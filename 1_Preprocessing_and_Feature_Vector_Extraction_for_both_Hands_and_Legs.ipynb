{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2118d987",
   "metadata": {},
   "source": [
    "---\n",
    "Setting File Paths\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21be4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output folders\n",
    "file_path_leg_validation = '/Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_val_leg_aug.pkl'\n",
    "file_path_leg_test = '/Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_test_leg_aug.pkl'\n",
    "file_path_leg_train = '/Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_leg_aug.pkl'\n",
    "file_path_hand_validation = '/Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_val_hand_aug.pkl'\n",
    "file_path_hand_test = '/Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_test_hand_aug.pkl'\n",
    "file_path_hand_train = '/Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_hand_aug.pkl'\n",
    "input_folder = '/Users/rishabhdubey/Downloads/FracAtlas/images/BothComplete'\n",
    "synthetic_folder = '/Users/rishabhdubey/Downloads/Project/Output_Synth'\n",
    "masked_synthetic_folder = '/Users/rishabhdubey/Downloads/Project/Masked_Synth'\n",
    "additional_folder = '/Users/rishabhdubey/Downloads/Project/Additional_Synth_new'\n",
    "json_path = '/Users/rishabhdubey/Downloads/Project/coordinates_and_rotations.json'\n",
    "original_folder = input_folder\n",
    "file_path = '/Users/rishabhdubey/Downloads/FracAtlas/dataset.csv'\n",
    "\n",
    "# Number of splits and folder \n",
    "max_nu_of_splits = 3\n",
    "masked_synthetic_folder = additional_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cbb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_leg_validation,  file_path_leg_test, file_path_leg_train, file_path_hand_validation, file_path_hand_test, file_path_hand_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96a3f5",
   "metadata": {},
   "source": [
    "---\n",
    "Importing Libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9fa084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import permutations\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, LeakyReLU, Conv1D, Flatten, Reshape, Lambda, Concatenate, MultiHeadAttention, LayerNormalization, Add, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "import cv2 as cv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "\n",
    "# Install keras-tuner\n",
    "!pip install keras-tuner --quiet\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Open and read the JSON file this contains each \n",
    "with open(json_path, 'r') as file:\n",
    "    coordinates_and_rotations = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a628c9",
   "metadata": {},
   "source": [
    "---\n",
    "# Creating X-ray splits\n",
    "---\n",
    "Creating augmented data. For each image 10 sets of labels (coordinates and rotatinos).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cb42bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1368076997d4d08832421a7f9e08b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Constants\n",
    "INITIAL_THRESHOLD = 220\n",
    "MIN_THRESHOLD = 70\n",
    "THRESHOLD_STEP = 10\n",
    "MIN_INTENSITY_PERCENTAGE = 0.05\n",
    "MAX_ATTEMPTS = 5\n",
    "STANDARD_SIZE = (512, 512)\n",
    "WINDOW_HEIGHT_RATIO_MIN = 0.9\n",
    "WINDOW_HEIGHT_RATIO_MAX = 1.0\n",
    "AUGMENTATION_COUNT = 10\n",
    "\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(synthetic_folder, exist_ok=True)\n",
    "os.makedirs(masked_synthetic_folder, exist_ok=True)\n",
    "os.makedirs(additional_folder, exist_ok=True)\n",
    "\n",
    "# to rotate the image by 90 degrees counterclockwise\n",
    "def rotate_90_anticlockwise(image):\n",
    "    return cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "# Function to crop edges\n",
    "def crop_edges(image, threshold=160, crop_percent=0.05):\n",
    "    h, w = image.shape[:2]\n",
    "    crop_pixels = int(crop_percent * min(h, w))\n",
    "    cropped_sides = []\n",
    "    while True:\n",
    "        top_edge = image[0, :]\n",
    "        bottom_edge = image[-1, :]\n",
    "        left_edge = image[:, 0]\n",
    "        right_edge = image[:, -1]\n",
    "        \n",
    "        if np.all(top_edge > threshold):\n",
    "            image = image[crop_pixels:, :]\n",
    "            cropped_sides.append('top')\n",
    "        if np.all(bottom_edge > threshold):\n",
    "            image = image[:-crop_pixels, :]\n",
    "            cropped_sides.append('bottom')\n",
    "        if np.all(left_edge > threshold):\n",
    "            image = image[:, crop_pixels:]\n",
    "            cropped_sides.append('left')\n",
    "        if np.all(right_edge > threshold):\n",
    "            image = image[:, :-crop_pixels]\n",
    "            cropped_sides.append('right')\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        if crop_pixels >= h or crop_pixels >= w:\n",
    "            break\n",
    "        \n",
    "        if not (np.all(top_edge > threshold) or np.all(bottom_edge > threshold) or np.all(left_edge > threshold) or np.all(right_edge > threshold)):\n",
    "            break\n",
    "    return image, cropped_sides\n",
    "\n",
    "# to split and validate image\n",
    "def split_and_validate(image, initial_threshold=INITIAL_THRESHOLD, min_intensity_percentage=MIN_INTENSITY_PERCENTAGE, max_attempts=MAX_ATTEMPTS):\n",
    "    h, w = image.shape[:2]\n",
    "    best_splits = None\n",
    "    best_threshold = initial_threshold\n",
    "\n",
    "    while initial_threshold >= MIN_THRESHOLD:\n",
    "        for _ in range(max_attempts):\n",
    "            x_coords = sorted([0, random.randint(w//3 - w//10, w//3 + w//10), random.randint(2*w//3 - w//10, 2*w//3 + w//10), w])\n",
    "            split_images = [image[:, x_coords[i]:x_coords[i+1]] for i in range(3)]\n",
    "            if all(np.sum(split >= initial_threshold) / split.size >= min_intensity_percentage for split in split_images):\n",
    "                return split_images, x_coords, initial_threshold\n",
    "        initial_threshold -= THRESHOLD_STEP\n",
    "\n",
    "    # Fallback to variance-based splitting\n",
    "    best_variance = float('inf')\n",
    "    for _ in range(max_attempts):\n",
    "        x_coords = sorted([0, random.randint(w//3 - w//10, w//3 + w//10), random.randint(2*w//3 - w//10, 2*w//3 + w//10), w])\n",
    "        split_images = [image[:, x_coords[i]:x_coords[i+1]] for i in range(3)]\n",
    "        variances = np.var([np.sum(split) for split in split_images])\n",
    "        if variances < best_variance:\n",
    "            best_variance = variances\n",
    "            best_splits = split_images\n",
    "            best_threshold = \"fallback\"\n",
    "\n",
    "    return best_splits, x_coords, best_threshold\n",
    "\n",
    "# to get maximum intensity window or the center adjusted for cropping\n",
    "def max_intensity_window(split, window_height_ratio):\n",
    "    h, w = split.shape[:2]\n",
    "    window_height = int(h * window_height_ratio)\n",
    "    crop_amount = h - window_height\n",
    "\n",
    "    if random.choice([True, False]):\n",
    "        # Crop from the top\n",
    "        split = split[crop_amount:, :]\n",
    "        y_center = (h // 2) + (crop_amount // 2)\n",
    "    else:\n",
    "        # Crop from the bottom\n",
    "        split = split[:-crop_amount, :]\n",
    "        y_center = (h // 2) - (crop_amount // 2)\n",
    "\n",
    "    return y_center, split\n",
    "\n",
    "# to rotate image randomly within +-10 degrees\n",
    "def random_rotate(image):\n",
    "    angle = random.uniform(-10, 10)\n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1)\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (w, h))\n",
    "    return rotated_image, angle\n",
    "\n",
    "# to apply circular mask\n",
    "def apply_circular_mask(image):\n",
    "    h, w = image.shape[:2]\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    center = (w // 2, h // 2)\n",
    "    radius = min(center) - 10\n",
    "    cv2.circle(mask, center, radius, 255, -1)\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    return masked_image\n",
    "\n",
    "# Main processing\n",
    "coordinates_and_rotations = []\n",
    "for filename in tqdm(os.listdir(input_folder)):\n",
    "    if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Rotate image by 90 degrees counterclockwise\n",
    "        image = rotate_90_anticlockwise(image)\n",
    "        \n",
    "        # Step 1: Crop edges\n",
    "        image, cropped_sides = crop_edges(image)\n",
    "        \n",
    "        for aug_num in range(1, AUGMENTATION_COUNT + 1):\n",
    "            aug_filename_prefix = f\"{aug_num}_\"\n",
    "            augmented_filename = aug_filename_prefix + filename\n",
    "            \n",
    "            # Step 2: Resize to 512x512\n",
    "            resized_image = cv2.resize(image, STANDARD_SIZE)\n",
    "            \n",
    "            # Step 3 and 4: Split and validate\n",
    "            splits, x_coords, used_threshold = split_and_validate(resized_image)\n",
    "            \n",
    "            split_data = {'filename': augmented_filename, 'splits': [], 'used_threshold': used_threshold}\n",
    "            for split_idx, split_image in enumerate(splits):\n",
    "                # Step 5: get y-coordinates\n",
    "                window_height_ratio = random.uniform(WINDOW_HEIGHT_RATIO_MIN, WINDOW_HEIGHT_RATIO_MAX)\n",
    "                y_center, split_image = max_intensity_window(split_image, window_height_ratio)\n",
    "                split_data['splits'].append({'x_center': (x_coords[split_idx] + x_coords[split_idx+1]) // 2, 'y_center': y_center})\n",
    "                \n",
    "                # Step 6: Rotate and save images\n",
    "                rotated_image, angle = random_rotate(split_image)\n",
    "                additional_filename = f\"{os.path.splitext(augmented_filename)[0]}_{split_idx+1}.jpg\"\n",
    "                additional_image_path = os.path.join(additional_folder, additional_filename)\n",
    "                cv2.imwrite(additional_image_path, rotated_image)\n",
    "                \n",
    "                # Step 7: Resize back to 512x512\n",
    "                split_image_resized = cv2.resize(split_image, STANDARD_SIZE)\n",
    "                output_filename = f\"{os.path.splitext(augmented_filename)[0]}_{split_idx+1}.jpg\"\n",
    "                output_image_path = os.path.join(synthetic_folder, output_filename)\n",
    "                cv2.imwrite(output_image_path, split_image_resized)\n",
    "                \n",
    "                # Step 8: Apply circular mask\n",
    "                masked_image = apply_circular_mask(split_image_resized)\n",
    "                masked_image_path = os.path.join(masked_synthetic_folder, output_filename)\n",
    "                cv2.imwrite(masked_image_path, masked_image)\n",
    "                \n",
    "                # data for JSON\n",
    "                split_data['splits'][split_idx]['rotation'] = angle\n",
    "            \n",
    "            coordinates_and_rotations.append(split_data)\n",
    "\n",
    "# Step 9: Saving coordinates and rotations to JSON\n",
    "with open(json_path, 'w') as json_file:\n",
    "    json.dump(coordinates_and_rotations, json_file, indent=4)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0900fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coordinates_and_rotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c21c42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the coordinates dict to store the results\n",
    "coordinates_d = {}\n",
    "\n",
    "# Populate the coordinates\n",
    "for item in coordinates_and_rotations:\n",
    "    filename = item['filename']\n",
    "    # Extract the (x, y) coordinates and convert them to integers\n",
    "    coordinates = [(int(split['x_center']), int(split['y_center'])) for split in item['splits']]\n",
    "    coordinates_d[filename] = coordinates\n",
    "    \n",
    "    \n",
    "    \n",
    "images = []\n",
    "for i in coordinates_d.keys():\n",
    "    images.append(i)\n",
    "\n",
    "\n",
    "# Initialize the rotations dict to store the results\n",
    "rotations_d = {}\n",
    "\n",
    "# Populate the coordinates\n",
    "for item in coordinates_and_rotations:\n",
    "    filename = item['filename']\n",
    "    # Extract coordinates and convert them to integers\n",
    "    rotations = [int(split['rotation']) for split in item['splits']]\n",
    "    rotations_d[filename] = rotations\n",
    "\n",
    "len(rotations_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a1d4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3800"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = [image.split('_',1)[1] for image in images]\n",
    "img = set(img)\n",
    "len(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1686df7",
   "metadata": {},
   "source": [
    "---\n",
    "### Filtering dataset to Leg and Hand X-Rays ( Using the Dataset.csv file from the Fracatlas folder, one hot Labeled for image type)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b5d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_id', 'hand', 'leg', 'hip', 'shoulder', 'mixed', 'hardware',\n",
       "       'multiscan', 'fractured', 'fracture_count', 'frontal', 'lateral',\n",
       "       'oblique'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n",
    "columns = df.columns \n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c726f714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1538"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists = {col: df[df[col] == 1]['image_id'].tolist() for col in columns}\n",
    "img_hand = list( (set(lists['hand'])))\n",
    "img_leg = list( (set(lists['leg'])))\n",
    "\n",
    "len(img_hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "263e57ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab0026",
   "metadata": {},
   "source": [
    "Pre-Trained Feature Vector extraction model using VitImageProcessor using Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4329264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the Vision Transformer model and feature extractor\n",
    "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# Move model to GPU if available\n",
    "if device.type == 'cuda':\n",
    "    model.to(device)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = cv2.resize(image, (224, 224))  # for Vision Transformer as it 224x224 input size\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)  # Converting grayscale to RGB\n",
    "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    # Move tensor to GPU if available\n",
    "    if device.type == 'cuda':\n",
    "        pixel_values = pixel_values.to(device)\n",
    "    return pixel_values\n",
    "\n",
    "def extract_features(model, image):\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(preprocessed_image)\n",
    "        # Use the output of the last hidden state\n",
    "        features = outputs.last_hidden_state[:, 0, :]  # [CLS] token representation\n",
    "    # Move the result back to CPU\n",
    "    return features.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0569bc",
   "metadata": {},
   "source": [
    "Pre-Trained Feature Vector extraction model using Inception model, Inception version V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7831ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pre-trained InceptionV3 model with aux_logits=True\n",
    "model = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Modify the model to output 2048-dimensional features by removing the classification head\n",
    "model.fc = nn.Identity()  # Removing the final classification layer to get features before classification\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# the preprocessing function\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Convert grayscale to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    image = preprocess(image)\n",
    "    image = image.unsqueeze(0)  # to Add batch dimension\n",
    "    image = image.to(device)\n",
    "    return image\n",
    "\n",
    "def extract_features(model, image):\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(preprocessed_image)\n",
    "        # If aux_logits are enabled, the model will outputs a tuple\n",
    "        if isinstance(outputs, tuple):\n",
    "            features = outputs[0]  # the main output, i am ignoring auxiliary outputs\n",
    "        else:\n",
    "            features = outputs\n",
    "    return features.cpu().numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151359cf",
   "metadata": {},
   "source": [
    "---\n",
    "### Train Test Validation split for Leg and creating feature vectors for the Leg data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c9151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2159\n",
      "Test set size: 57\n",
      "Validation set size: 57\n"
     ]
    }
   ],
   "source": [
    "training, temping = train_test_split(img_leg, test_size=0.05, random_state=42)\n",
    "testing, validating = train_test_split(temping, test_size=0.5, random_state=42)\n",
    "# Check the size of each set\n",
    "print(f\"Train set size: {len(training)}\")\n",
    "print(f\"Test set size: {len(testing)}\")\n",
    "print(f\"Validation set size: {len(validating)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5121362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "016cb73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e26b937240452e8c19cbaa38e5f10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting test features:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_dict_vision_test = {}\n",
    "\n",
    "file_list = [filename for filename in os.listdir(original_folder) if (filename.endswith('.png') or filename.endswith('.jpg')) and filename in testing]\n",
    "\n",
    "# Loop for each original image\n",
    "for filename in tqdm(file_list, desc=\"Extracting test features\"):\n",
    "    # Read split images\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    for i in range(1, max_nu_of_splits+1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"6_{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error: {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            # features extracted for split image\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # features\n",
    "    features_dict_vision_test[filename] = split_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f55a1d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test features with InceptionV3: 100%|█| 57/57 [00:10<00:00,  5.33it/s\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "features_dict_vision_test_inception = {}\n",
    "\n",
    "# List of image files to process (from the testing set)\n",
    "file_list = [filename for filename in os.listdir(original_folder) if (filename.endswith('.png') or filename.endswith('.jpg')) and filename in testing]\n",
    "\n",
    "# Loop for each original image\n",
    "for filename in tqdm(file_list, desc=\"Extracting test features with InceptionV3\"):\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    \n",
    "    for i in range(1, max_nu_of_splits + 1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"6_{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error reading {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            # Extract features for the split image\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Store features in dict\n",
    "    features_dict_vision_test_inception[filename] = split_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "554bc9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features: 100%|███████████| 57/57 [00:10<00:00,  5.36it/s]\n"
     ]
    }
   ],
   "source": [
    "features_dict_vision_val = {}\n",
    "\n",
    "file_list = [filename for filename in os.listdir(original_folder) if (filename.endswith('.png') or filename.endswith('.jpg')) and filename in validating]\n",
    "\n",
    "# Loop for each original image\n",
    "for filename in tqdm(file_list, desc=\"Extracting validation features\"):\n",
    "    # Read split images\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    for i in range(1, max_nu_of_splits+1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"6_{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error: {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            # features extracted for split image\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # features \n",
    "    features_dict_vision_val[filename] = split_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e84ca94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features with InceptionV3: 100%|█| 57/57 [00:08<00:00,  6.\n"
     ]
    }
   ],
   "source": [
    "features_dict_vision_val_inception = {}\n",
    "\n",
    "file_list_val = [filename for filename in os.listdir(original_folder) if (filename.endswith('.png') or filename.endswith('.jpg')) and filename in validating]\n",
    "\n",
    "for filename in tqdm(file_list_val, desc=\"Extracting validation features with InceptionV3\"):\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    for i in range(1, max_nu_of_splits+1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"6_{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error: {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    features_dict_vision_val_inception[filename] = split_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3636f8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2159"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77a798ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [filename for filename in os.listdir(original_folder) if (filename.endswith('.png') or filename.endswith('.jpg')) and filename in training]\n",
    "new_file_list = []\n",
    "\n",
    "# Outer loop for each file in the original file_list\n",
    "for file in file_list:\n",
    "    # Inner loop to add prefixes from '1_' to '10_'\n",
    "    for i in range(1, 11):\n",
    "        new_file_list.append(f\"{i}_{file}\")\n",
    "file_list = new_file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8f415a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting training features: 100%|███████| 21590/21590 [54:32<00:00,  6.60it/s]\n"
     ]
    }
   ],
   "source": [
    "features_dict_vision = {}\n",
    "\n",
    "# Loop for each original image\n",
    "for filename in tqdm(file_list, desc=\"Extracting training features\"):\n",
    "    # Read split images\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    for i in range(1, max_nu_of_splits+1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error: {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            # features extracted for split image\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # features in dict\n",
    "    features_dict_vision[filename] = split_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "922c203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features with InceptionV3: 100%|█| 21590/21590 [00:00<00:0\n"
     ]
    }
   ],
   "source": [
    "features_dict_vision_inception = {}\n",
    "\n",
    "for filename in tqdm(file_list, desc=\"Extracting validation features with InceptionV3\"):\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    for i in range(1, max_nu_of_splits+1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"6_{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error: {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    features_dict_vision_inception[filename] = split_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b04cd",
   "metadata": {},
   "source": [
    "Writing the Feature vectors to the Drive as a .pickle file, as they maintatin the structure when will be used in other notebook for Leg models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e853fdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leg Validation feature vectors saved to /Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_val_leg_aug.pkl\n",
      "Leg Test feature vectors saved to /Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_test_leg_aug.pkl\n",
      "Leg Training Feature vectors saved to /Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_leg_aug.pkl\n"
     ]
    }
   ],
   "source": [
    "# Write the Leg feature vectors to a pickle file\n",
    "with open(file_path_leg_validation, 'wb') as pickle_file:\n",
    "    pickle.dump(features_dict_vision_val, pickle_file)\n",
    "\n",
    "print(f\"Leg Validation feature vectors saved to {file_path_leg_validation}\")\n",
    "\n",
    "\n",
    "with open(file_path_leg_test, 'wb') as pickle_file:\n",
    "    pickle.dump(features_dict_vision_test, pickle_file)\n",
    "\n",
    "print(f\"Leg Test feature vectors saved to {file_path_leg_test}\")\n",
    "\n",
    "with open(file_path_leg_train, 'wb') as pickle_file:\n",
    "    pickle.dump(features_dict_vision, pickle_file)\n",
    "\n",
    "print(f\"Leg Training Feature vectors saved to {file_path_leg_train}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bfeac1",
   "metadata": {},
   "source": [
    "---\n",
    "### Train Test Validation split for Leg and creating feature vectors for the Hand data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "577ce88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1461\n",
      "Test set size: 38\n",
      "Validation set size: 39\n"
     ]
    }
   ],
   "source": [
    "training, temping = train_test_split(img_hand, test_size=0.05, random_state=42)\n",
    "testing, validating = train_test_split(temping, test_size=0.5, random_state=42)\n",
    "# size check of each set\n",
    "print(f\"Train set size: {len(training)}\")\n",
    "print(f\"Test set size: {len(testing)}\")\n",
    "print(f\"Validation set size: {len(validating)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d429c412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test features with Vision Transformer: 100%|█| 38/38 [00:05<00:00,  7\n"
     ]
    }
   ],
   "source": [
    "features_dict_vision_test = {}\n",
    "\n",
    "file_list = [filename for filename in os.listdir(original_folder) if (filename.endswith('.png') or filename.endswith('.jpg')) and filename in testing]\n",
    "\n",
    "# Loop for each original image\n",
    "for filename in tqdm(file_list, desc=\"Extracting test features with Vision Transformer\"):\n",
    "    # Read split images\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    for i in range(1, max_nu_of_splits+1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"6_{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error: {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            # features extracted for split image\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # features in dict\n",
    "    features_dict_vision_test[filename] = split_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3f3b8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test features with InceptionV3: 100%|█| 38/38 [00:04<00:00,  7.62it/s\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "features_dict_vision_test_inception = {}\n",
    "\n",
    "# List of image files to process (from the testing set)\n",
    "file_list = [filename for filename in os.listdir(original_folder) if (filename.endswith('.png') or filename.endswith('.jpg')) and filename in testing]\n",
    "\n",
    "# Loop for each original image\n",
    "for filename in tqdm(file_list, desc=\"Extracting test features with InceptionV3\"):\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    \n",
    "    for i in range(1, max_nu_of_splits + 1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"6_{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error reading {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            # Extract features for the split image\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Store features in dictionary\n",
    "    features_dict_vision_test_inception[filename] = split_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da019661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features with Vision Transformer: 100%|█| 39/39 [00:05<00:\n"
     ]
    }
   ],
   "source": [
    "features_dict_vision_val = {}\n",
    "\n",
    "file_list = [filename for filename in os.listdir(original_folder) if (filename.endswith('.png') or filename.endswith('.jpg')) and filename in validating]\n",
    "\n",
    "# Loop for each original image\n",
    "for filename in tqdm(file_list, desc=\"Extracting validation features with Vision Transformer\"):\n",
    "    # Read split images\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    for i in range(1, max_nu_of_splits+1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"6_{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error: {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            # features extracted for split image\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # features in dict\n",
    "    features_dict_vision_val[filename] = split_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d43db36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features with InceptionV3: 100%|█| 39/39 [00:05<00:00,  7.\n"
     ]
    }
   ],
   "source": [
    "features_dict_vision_val_inception = {}\n",
    "\n",
    "file_list_val = [filename for filename in os.listdir(original_folder) if (filename.endswith('.png') or filename.endswith('.jpg')) and filename in validating]\n",
    "\n",
    "for filename in tqdm(file_list_val, desc=\"Extracting validation features with InceptionV3\"):\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    for i in range(1, max_nu_of_splits+1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"6_{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error: {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    features_dict_vision_val_inception[filename] = split_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40e4f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [filename for filename in os.listdir(original_folder) if (filename.endswith('.png') or filename.endswith('.jpg')) and filename in training]\n",
    "new_file_list = []\n",
    "\n",
    "# Outer loop for each file in the original file_list\n",
    "for file in file_list:\n",
    "    # Inner loop to add prefixes from '1_' to '10_'\n",
    "    for i in range(1, 11):\n",
    "        new_file_list.append(f\"{i}_{file}\")\n",
    "file_list = new_file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c4a1263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14580"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad96d519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting training features with Vision Transformer: 100%|█| 14580/14580 [40:24\n"
     ]
    }
   ],
   "source": [
    "features_dict_vision = {}\n",
    "# Loop for each original imaget\n",
    "for filename in tqdm(file_list, desc=\"Extracting training features with Vision Transformer\"):\n",
    "    # Read split images\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    for i in range(1, max_nu_of_splits+1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error: {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            # features extracted for split image\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # features\n",
    "    features_dict_vision[filename] = split_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6ad60d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features with InceptionV3: 100%|█| 14580/14580 [00:00<00:0\n"
     ]
    }
   ],
   "source": [
    "features_dict_vision_inception = {}\n",
    "\n",
    "for filename in tqdm(file_list, desc=\"Extracting validation features with InceptionV3\"):\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    split_images = []\n",
    "    split_features = []\n",
    "    for i in range(1, max_nu_of_splits+1):\n",
    "        split_image_path = os.path.join(masked_synthetic_folder, f\"6_{base_name}_{i}.jpg\")\n",
    "        if os.path.exists(split_image_path):\n",
    "            split_image = cv2.imread(split_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if split_image is None:\n",
    "                print(f\"Error: {split_image_path}\")\n",
    "                continue\n",
    "            split_images.append(split_image)\n",
    "            split_features.append(extract_features(model, split_image))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    features_dict_vision_inception[filename] = split_features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0332777",
   "metadata": {},
   "source": [
    "---\n",
    "Writing the Feature vectors to the Drive as a .pickle file, as they maintatin the structure when will be used in other notebook for Leg models Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee241fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hand Validation Feature vectors  saved to /Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_val_hand_aug.pkl\n",
      "Hand Test Feature vectors  saved to /Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_test_hand_aug.pkl\n",
      "Hand Training Feature vectors saved to /Users/rishabhdubey/Downloads/Project/Feature Vectors/17th/features_dict_vision_hand_aug.pkl\n"
     ]
    }
   ],
   "source": [
    "# Write the hand feature vectors to a pickle file\n",
    "with open(file_path_hand_validation, 'wb') as pickle_file:\n",
    "    pickle.dump(features_dict_vision_val, pickle_file)\n",
    "\n",
    "print(f\"Hand Validation Feature vectors  saved to {file_path_hand_validation}\")\n",
    "\n",
    "with open(file_path_hand_test, 'wb') as pickle_file:\n",
    "    pickle.dump(features_dict_vision_test, pickle_file)\n",
    "\n",
    "print(f\"Hand Test Feature vectors  saved to {file_path_hand_test}\")\n",
    "\n",
    "\n",
    "with open(file_path_hand_train, 'wb') as pickle_file:\n",
    "    pickle.dump(features_dict_vision, pickle_file)\n",
    "\n",
    "print(f\"Hand Training Feature vectors saved to {file_path_hand_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca243869",
   "metadata": {},
   "source": [
    "---\n",
    "Writing the Json Object Containing Labels for the Image Data to Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ed391b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, 'w') as file:\n",
    "    json.dump(coordinates_and_rotations, file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5844cae",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
